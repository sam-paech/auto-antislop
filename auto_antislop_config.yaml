# Main Auto-Antislop Configuration

# --- Experiment Setup ---
experiment_base_dir: "results/auto_antislop_runs" # Base for timestamped run directories
human_profile_path: "data/human_writing_profile.json"
log_level: "INFO"

# --- vLLM Server Management (if --manage-vllm is True) ---
manage_vllm: true
vllm_model_id: "unsloth/gemma-3-12b-it" # Model served by vLLM
vllm_port: 8000
vllm_hf_token: null # Optional: Your Hugging Face token if model is gated
vllm_cuda_visible_devices: "0"  # set to e.g. "0,1,2,3" for multiple gpus
vllm_gpu_memory_utilization: 0.85 # leave some room for the refusal classifier if you are using it (about 3gb)
vllm_max_model_len: 2500
vllm_dtype: "bfloat16"
# Additional raw CLI arguments for vLLM server, e.g., ["--tensor-parallel-size", "4"] for multiple gpus
vllm_extra_args: []


# --- Iterative Anti-Slop Pipeline ---

# Iteration 0: Generates the baseline dataset & computes slop strings/ngrams to ban
# Iteration 1: Generates a dataset using antislop, banning those strings & ngrams. Recomputes the slop strings/ngrams at the end & adds any new slop to the ban lists
# Iteration 2+: Same as 1
num_iterations: 2 # Minimum 2 iterations (this is enough to catch most slop)

# --- Generation Script (antislop-vllm/main.py) Parameters ---
generation_step_enabled: true
# If you set manage_vllm=true, leave the base url unset
#generation_api_base_url: "http://localhost:8000/v1"
#generation_api_base_url: "https://apjmbtwbrb8t61-8888.proxy.runpod.net/v1"
generation_model_id: "unsloth/gemma-3-12b-it" # Model name for generation requests
generation_api_key: "xxx" # API key for the vLLM server
generation_max_new_tokens: 2000
generation_threads: 50 # Number of parallel threads for API queries in antislop-vllm. Note: vllm can become very inefficient if you go over some concurrency threshold (depending on vram)
generation_max_prompts: 300 # Number of samples to generate from the prompts in the dataset
generation_hf_dataset_name: 'Nitral-AI/Reddit-SFW-Writing_Prompts_ShareGPT'
generation_hf_dataset_split: 'train'
generation_logging_level: 'INFO' # Logging level for antislop-vllm script

# A huggingface model id or local dir containing the tokeniser you want to use to apply chat templates.
# This is important if you are generating a tdpo dataset for later training.
generation_chat_template_model_id: "unsloth/gemma-3-12b-it" 

# Parameters for antislop-vllm's generation_params (passed as individual CLI args)
generation_param_chunk_size: 20
generation_param_top_logprobs_count: 20
generation_param_temperature: 1
generation_param_top_p: 1.0
generation_param_top_k: 50
generation_param_min_p: 0.03
generation_param_timeout: 480
generation_param_stop_sequences: [] # e.g., ["\n\n", "---"]

# The prompt template wraps the prompts when generating from a dataset.
# To use the original prompt exactly, set the template to "{prompt}"
generation_prompt_template: "Writing prompt: {prompt}\n\nWrite 1000 words to this prompt. Your response:\n"
generation_system_prompt: ""        # optional; left empty → no system prompt

# If set to true:
  #   when resampling after backtracking, if we don't find a valid replacement token
  #   we progressively disable sampling options (temp, then min_p, then top_p, then top_k)
  #   until we find a non-banned replacement or run out of candidates.
generation_force_backtrack: false

# Inverts the probability distribution after other sampling modifications have been applied
  #    This encourages selection of the tail of the top n candidates, for more diverse outputs.
  #    You should probably only use this if using min_p to constrain the distribution.
  #    otherwise you will likely get incoherent completions.
generation_invert_probs: true

# Parameters for antislop-vllm's ngram_validator (passed as individual CLI args)
# N-gram ban list file is managed by auto-antislop's iterative process.
generation_ngram_remove_stopwords: true
generation_ngram_language: "english"

# Detects refusals & doesn't include them in the training dataset. Uses about 3GB extra VRAM.
generation_refusal_detection: true


# --- N-Gram Analysis & Banning (within auto-antislop) ---
enable_ngram_ban: true
top_k_bigrams: 5000
top_k_trigrams: 5000
# Quotas for adding n-grams to ban list per iteration
dict_bigrams_initial: 400
dict_bigrams_subsequent: 70
nodict_bigrams_initial: 800
nodict_bigrams_subsequent: 100
dict_trigrams_initial: 300
dict_trigrams_subsequent: 50
nodict_trigrams_initial: 800
nodict_trigrams_subsequent: 100
# User-supplied extra n-grams to always ban (processed by auto-antislop)
extra_ngrams_to_ban: [
  # "voice barely whisper",
]

# --- Over-Represented Word Analysis & Banning (within auto-antislop) ---
compute_overrep_words: true
top_k_words_for_overrep_analysis: 200000
# Quotas for adding over-represented words to slop phrase ban list
dict_overrep_initial: 800
dict_overrep_subsequent: 200
nodict_overrep_initial: 80
nodict_overrep_subsequent: 20

# --- Slop Phrase Banning (within auto-antislop) ---
enable_slop_phrase_ban: true
ban_overrep_words_in_phrase_list: true # Add selected over-rep words to slop phrase list
min_phrase_freq_to_keep: 2 # Min frequency for a new phrase from slop-forensics to be considered
top_n_initial_slop_ban: 600 # New slop phrases from slop-forensics to ban in iter 0
top_n_subsequent_slop_ban: 100 # New slop phrases from slop-forensics to ban in later iters
# User-supplied extra slop phrases to always ban (processed by auto-antislop)
extra_slop_phrases_to_ban: [
  # "testament to",
  "…", "*", " –", "–", "#",
]

# --- Regex Banning (within auto-antislop, passed to antislop-vllm) ---
# User-supplied regex patterns to ban
extra_regex_patterns: [
    # These ones ban "it's not x, it's y" type patterns:
    "\\bnot\\s+(?:just|only|merely)?\\s*(?:(?!but\\b|[.?!]).){1,50}?[,;:—–-]?\\s*but\\s+(?:also\\s+)?",
    "\\bnot\\s+only\\s+(?:(?!but\\b|[.?!]).){1,50}?[,;:—–-]?\\s*but\\s+also\\s+",
    "\\bit'?s\\s+not\\s+(?:just|only|merely)?\\s*(?:(?!it'?s\\b|[.?!]).){1,50}?[,;:—–-]\\s*it'?s\\s+",
    "\\b(?:(?!is\\b|[.?!]).){1,50}?is\\s+not\\s+(?:just\\s+|only\\s+)?(?:about\\s+)?(?:(?!it'?s\\b|[.?!]).){1,50}?[,;:—–-]\\s*it'?s\\s+(?:about\\s+)?"
]

# --- Metrics ---
min_word_len_for_analysis: 3 # For n-gram analysis and lexical diversity
freq_norm_denom_for_analysis: 100000 # For normalizing frequencies
top_n_repetition_stat: 50 # N-grams from each category to track for repetition stats

# --- Finetuning (Optional) ---
finetune_enabled: true
finetune_use_unsloth: false
finetune_mode: "tdpo-multi" # dpo / tdpo (tokenwise-dpo)
finetune_tdpo_dataset: "" # you can specify an existing tdpo dataset, or let the pipeline use the one produced in the generation step
finetune_base_model_id: "unsloth/gemma-3-12b-it" # Base model for DPO
finetune_max_seq_length: 1500 # this may truncate some outputs
finetune_load_in_4bit: false
finetune_early_stopping_wins: 0.8 # Early stopping threshold for fraction of *chosen* completions that are selected over *rejected*. More than 0.75 may be overtrained.
finetune_early_stopping_loss: null # Loss threshold for early stopping. Set to null to disable.
finetune_lora_r: 8
finetune_lora_alpha: 8
finetune_lora_dropout: 0.05
finetune_weight_decay: 0.01
finetune_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
finetune_gradient_checkpointing: "unsloth"
finetune_chat_template: "" # e.g. "gemma-3" -- get the chat template from unsloth's helper if required, otherwise leave the string blank to use the tokeniser's chat template
finetune_batch_size: 1
finetune_gradient_accumulation_steps: 16
finetune_warmup_ratio: 0.1
finetune_num_epochs: 1
finetune_learning_rate: 0.000001
finetune_auto_learning_rate: true  # true: automatically determine learning rate based on dataset size, effective batch size & lora rank
finetune_auto_learning_rate_adjustment_scaling: 0.2 # scale the auto-lr by this factor
finetune_beta: 0.1 # DPO beta
finetune_output_dir_suffix: "_tdpo_exp01" # Appended to experiment run dir
finetune_save_merged_16bit: true
finetune_save_gguf_q8_0: false
finetune_max_train_examples: 10000 # adjust as needed
# --- TDPO sample regularisation
# 0 = off; 0.9 strongly downsamples overrepresented rule violations
# (this is useful because the raw generated dataset is typically very skewed)
finetune_tdpo_sample_regularisation_strength: 0.8
