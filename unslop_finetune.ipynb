{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda72f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Gemma3_(4B)_DPO.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb\n",
    "\"\"\"\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %%capture\n",
    "# import os\n",
    "# if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "#     print(\"Not in Colab. Assuming Unsloth is already installed.\")\n",
    "#     print(\"If not, please install with: pip install unsloth[colab-new]\")\n",
    "# else:\n",
    "#     # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "#     print(\"Installing Unsloth and dependencies for Colab...\")\n",
    "#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "#     !pip install --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "#     print(\"Installation complete.\")\n",
    "\n",
    "from unsloth import FastLanguageModel # Changed from FastModel for DPO consistency\n",
    "import torch\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "max_seq_length = 2048 # Choose any for long context!\n",
    "# Use \"unsloth\" for LoRA optimization to fit 2x larger batch sizes!\n",
    "use_gradient_checkpointing = \"unsloth\" # True or \"unsloth\" for very long context\n",
    "\n",
    "# Load Gemma-3 model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\", # Using the instruction-tuned base\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    "    # You can add dtype=torch.bfloat16 if your GPU supports it for potentially faster training\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # LoRA rank. 8-64 is common. DPO template used 64. Let's try 16.\n",
    "    lora_alpha = 32, # Recommended lora_alpha = 2 * r\n",
    "    lora_dropout = 0.05, # Supports any, but = 0 is optimized for some Unsloth features\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # Target modules for Gemma-3. These are typical for Llama-like architectures.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    use_gradient_checkpointing = use_gradient_checkpointing,\n",
    "    random_state = 3407,\n",
    "    max_seq_length = max_seq_length,\n",
    ")\n",
    "\n",
    "# Set up chat template for Gemma-3\n",
    "# DPOTrainer will use this to format prompts and responses\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    "    # map_eos_token = True, # Gemma-3 tokenizer has an EOS token, so this might not be needed\n",
    ")\n",
    "# It's good practice to set pad_token if it's not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# --- Data Prep for DPO ---\n",
    "# DPO requires a dataset with 'prompt', 'chosen', and 'rejected' columns.\n",
    "# Replace this with your actual DPO dataset.\n",
    "# Example using a small, standard DPO dataset:\n",
    "# For a real run, you'd use a larger, more relevant dataset.\n",
    "# Popular options:\n",
    "#   - \"HuggingFaceH4/ultrafeedback_binarized-preferences-cleaned\" (large, high quality)\n",
    "#   - \"argilla/ultrafeedback-binarized-preferences-cleaned-ja\" (Japanese version)\n",
    "#   - \"trl-internal-testing/hh-rlhf-trl-style\" (small, for testing)\n",
    "\n",
    "# For demonstration, let's create a tiny dummy dataset\n",
    "# In a real scenario, you would load your DPO dataset:\n",
    "# dpo_dataset = load_dataset(\"your_username/your_dpo_dataset_name\", split=\"train\")\n",
    "# Or from a local file:\n",
    "# dpo_dataset = load_dataset(\"json\", data_files=\"path/to/your/dpo_data.jsonl\", split=\"train\")\n",
    "\n",
    "# Using a small example dataset for demonstration\n",
    "# This dataset has 'prompt', 'chosen', 'rejected' fields\n",
    "USE_LOCAL_DATASET = True\n",
    "\n",
    "if USE_LOCAL_DATASET:\n",
    "    # --- Data Prep for DPO -------------------------------------------------------\n",
    "    # set to the dpo pairs dataset that was generated by auto_unslop\n",
    "    dpo_file = \"./results/[experiment_id]/dpo_pairs_dataset.jsonl\"\n",
    "    print(f\"Loading DPO dataset from {dpo_file}\")\n",
    "    dpo_dataset = load_dataset(\"json\", data_files=str(dpo_file), split=\"train\")\n",
    "\n",
    "    # quick sanity-check – drop any rows that somehow lack the three fields\n",
    "    req_cols = {\"prompt\", \"chosen\", \"rejected\"}\n",
    "    before   = len(dpo_dataset)\n",
    "    dpo_dataset = dpo_dataset.filter(lambda x: all(col in x and x[col] for col in req_cols))\n",
    "    after    = len(dpo_dataset)\n",
    "    if after == 0:\n",
    "        raise ValueError(\"All rows were filtered out – check dataset contents.\")\n",
    "    if after < before:\n",
    "        print(f\"Filtered out {before - after} malformed rows; {after} remain.\")\n",
    "\n",
    "    print(f\"DPO dataset ready with {after} samples.\")\n",
    "else:\n",
    "    dpo_dataset = load_dataset(\"trl-internal-testing/hh-rlhf-trl-style\", split=\"train[:1%]\") # Take a small slice for quick demo\n",
    "    # Filter out examples where prompt, chosen, or rejected are None or empty\n",
    "    dpo_dataset = dpo_dataset.filter(\n",
    "        lambda x: x[\"prompt\"] is not None and x[\"chosen\"] is not None and x[\"rejected\"] is not None and \\\n",
    "                  len(x[\"prompt\"]) > 0 and len(x[\"chosen\"]) > 0 and len(x[\"rejected\"]) > 0\n",
    "    )\n",
    "    if len(dpo_dataset) == 0:\n",
    "        raise ValueError(\"Filtered dataset is empty. Check the original data or filtering logic.\")\n",
    "    print(f\"Loaded DPO dataset with {len(dpo_dataset)} samples.\")\n",
    "    print(\"First DPO sample:\")\n",
    "    print(dpo_dataset[0])\n",
    "\n",
    "\n",
    "# --- Train the model with DPO ---\n",
    "# Note: For DPO, you typically don't need a separate eval_dataset during training,\n",
    "# but it can be useful for monitoring.\n",
    "# The `ref_model` is set to None for LoRA, DPOTrainer will handle creating a reference.\n",
    "\n",
    "# Ensure the dataset is not empty\n",
    "if len(dpo_dataset) == 0:\n",
    "    raise ValueError(\"DPO dataset is empty. Please provide a valid dataset.\")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None, # Handled automatically by DPOTrainer for LoRA\n",
    "    train_dataset = dpo_dataset,\n",
    "    # eval_dataset = YOUR_EVAL_DPO_DATASET_HERE, # Optional\n",
    "    tokenizer = tokenizer,\n",
    "    args = DPOConfig(\n",
    "        per_device_train_batch_size = 1, # Adjust based on your GPU memory\n",
    "        gradient_accumulation_steps = 4, # Effective batch size = 1 * 4 = 4\n",
    "        warmup_ratio = 0.1, # Or warmup_steps\n",
    "        num_train_epochs = 1, # For a quick demo. Set to 1-3 for a real run.\n",
    "        # max_steps = 60, # Alternatively, use max_steps for a fixed number of steps\n",
    "        learning_rate = 5e-5, # Common DPO learning rate\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\", # Unsloth optimizes this\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs_gemma3_4b_dpo\",\n",
    "        max_length = max_seq_length,         # Max length of combined prompt + response\n",
    "        max_prompt_length = max_seq_length // 2, # Max length of prompt\n",
    "        beta = 0.1, # DPO beta parameter\n",
    "        report_to = \"none\", # \"wandb\" or \"tensorboard\"\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        # bf16 = True, # Set to True if your GPU supports bfloat16 and you loaded model with bfloat16\n",
    "        # fp16 = False, # Set to True for mixed precision if bf16 is not available (and not using 4bit)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Starting DPO training...\")\n",
    "trainer_stats = dpo_trainer.train()\n",
    "print(\"DPO training finished.\")\n",
    "\n",
    "\n",
    "# --- Show current memory stats (optional) ---\n",
    "if torch.cuda.is_available():\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.memory_reserved() / 1024 / 1024 / 1024, 3) # Corrected: use memory_reserved\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved after training.\") # This will be peak if called after train\n",
    "\n",
    "    # Show final memory and time stats\n",
    "    if hasattr(trainer_stats, 'metrics') and 'train_runtime' in trainer_stats.metrics:\n",
    "        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        # Note: start_gpu_memory was captured *after* model loading.\n",
    "        # A more accurate \"used_memory_for_lora\" would require capturing memory before model load and after.\n",
    "        # For simplicity, we'll just show peak reserved.\n",
    "        used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "        print(f\"{trainer_stats.metrics['train_runtime']:.2f} seconds used for training.\")\n",
    "        print(f\"{trainer_stats.metrics['train_runtime']/60:.2f} minutes used for training.\")\n",
    "        print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "        print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "else:\n",
    "    print(\"CUDA not available. Memory stats not shown.\")\n",
    "\n",
    "\n",
    "# --- Inference after DPO ---\n",
    "# For inference, make sure the tokenizer has the chat template for Gemma-3\n",
    "# The model is already LoRA-adapted.\n",
    "\n",
    "# Reload tokenizer with chat template if needed (should be already set)\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template = \"gemma-3\",\n",
    "# )\n",
    "\n",
    "# Example prompt\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What are the pros and cons of pineapple on pizza?\"\n",
    "}]\n",
    "\n",
    "# Apply chat template for generation\n",
    "# For DPO, the model has learned from preferences, so it should generate better responses.\n",
    "# The prompt format should match what it saw during training (user turn).\n",
    "# `add_generation_prompt=True` adds the necessary tokens to signal the model to start generating.\n",
    "# For Gemma-3, this means it will end with `<start_of_turn>model\\n`\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Crucial for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "\n",
    "print(\"\\n--- Generating DPO model response (streaming) ---\")\n",
    "_ = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens = 128, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 0.7, # Slightly lower temperature for more focused output after DPO\n",
    "    top_p = 0.95,\n",
    "    top_k = 64,\n",
    "    streamer = text_streamer,\n",
    "    pad_token_id = tokenizer.eos_token_id # Important for generation\n",
    ")\n",
    "print(\"\\n--- End of DPO model response ---\")\n",
    "\n",
    "# --- Saving the DPO-finetuned model (LoRA adapters) ---\n",
    "dpo_model_save_path = \"gemma-3-4b-dpo-lora\"\n",
    "dpo_trainer.save_model(dpo_model_save_path) # Saves LoRA adapters\n",
    "tokenizer.save_pretrained(dpo_model_save_path)\n",
    "print(f\"DPO LoRA adapters and tokenizer saved to ./{dpo_model_save_path}\")\n",
    "\n",
    "# To load the LoRA adapters later for inference:\n",
    "if False: # Set to True to test loading\n",
    "    from unsloth import FastLanguageModel\n",
    "    loaded_model, loaded_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = dpo_model_save_path, # Path to your saved LoRA adapters\n",
    "        max_seq_length = max_seq_length,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "    # Now `loaded_model` is ready for inference.\n",
    "    # Ensure tokenizer has chat template\n",
    "    loaded_tokenizer = get_chat_template(\n",
    "        loaded_tokenizer,\n",
    "        chat_template = \"gemma-3\",\n",
    "    )\n",
    "    if loaded_tokenizer.pad_token is None:\n",
    "        loaded_tokenizer.pad_token = loaded_tokenizer.eos_token\n",
    "\n",
    "    print(\"\\n--- Generating response from loaded DPO LoRA model (streaming) ---\")\n",
    "    _ = loaded_model.generate(\n",
    "        inputs, # Using the same inputs as before\n",
    "        max_new_tokens = 128,\n",
    "        temperature = 0.7, top_p = 0.95, top_k = 64,\n",
    "        streamer = TextStreamer(loaded_tokenizer, skip_prompt = True),\n",
    "        pad_token_id = loaded_tokenizer.eos_token_id\n",
    "    )\n",
    "    print(\"\\n--- End of loaded DPO model response ---\")\n",
    "\n",
    "\n",
    "# --- Saving to float16 for VLLM or other deployments (merged model) ---\n",
    "if False: # Change to True to save merged finetune!\n",
    "    # Merges LoRA adapters into the base model and saves\n",
    "    # This creates a standalone model, not just adapters\n",
    "    merged_model_path = \"gemma-3-4b-dpo-merged\"\n",
    "    model.save_pretrained_merged(merged_model_path, tokenizer, save_method = \"merged_16bit\")\n",
    "    print(f\"Merged 16-bit DPO model saved to ./{merged_model_path}\")\n",
    "    # For GGUF, you can then convert this merged model or use Unsloth's direct GGUF saving\n",
    "    # model.push_to_hub_merged(\"YOUR_HF_USERNAME/gemma-3-4b-dpo-merged\", tokenizer, save_method = \"merged_16bit\", token = \"YOUR_HF_TOKEN\")\n",
    "\n",
    "# --- GGUF / llama.cpp Conversion ---\n",
    "if False: # Change to True to save to GGUF\n",
    "    # Saves the LoRA model directly to GGUF by first merging.\n",
    "    # Quantization types: \"Q8_0\", \"F16\", \"BF16\", \"Q4_K_M\", \"Q5_K_M\" etc.\n",
    "    gguf_model_path = \"gemma-3-4b-dpo-gguf\"\n",
    "    model.save_pretrained_gguf(gguf_model_path, tokenizer, quantization_method = \"q8_0\")\n",
    "    print(f\"GGUF (Q8_0) DPO model saved to ./{gguf_model_path}.gguf\") # Unsloth adds .gguf\n",
    "    # model.push_to_hub_gguf(\"YOUR_HF_USERNAME/gemma-3-4b-dpo-gguf\", tokenizer, quantization_method = \"q8_0\", token = \"YOUR_HF_TOKEN\")\n",
    "\n",
    "print(\"\\nScript finished. Remember to replace placeholder DPO dataset with your actual data.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
