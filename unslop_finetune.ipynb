{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d7904-05e3-46c9-9e54-a647a5347287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.553 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `base_model.model.vision_tower.vision_model` require gradients\n",
      "Loading DPO dataset from ./dpo_pairs_dataset.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae231dacd644af5aed13692af829510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcaafc68a8048f2b9051388847dff93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO dataset ready with 320 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fd6c1410544f6bb0758f6c672b9208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset (num_proc=48):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34797fcab764f6280b16387671328f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=48):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e315a55ae854302a9b638603a635db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=48):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 320 | Num Epochs = 3 | Total steps = 240\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 32,788,480/4,000,000,000 (0.82% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 23:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>eval_logits / chosen</th>\n",
       "      <th>eval_logits / rejected</th>\n",
       "      <th>nll_loss</th>\n",
       "      <th>aux_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.902900</td>\n",
       "      <td>-0.283318</td>\n",
       "      <td>0.889211</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-1.172529</td>\n",
       "      <td>-2506.524902</td>\n",
       "      <td>-2402.157471</td>\n",
       "      <td>-4.831528</td>\n",
       "      <td>-4.792796</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.695400</td>\n",
       "      <td>3.861495</td>\n",
       "      <td>2.541639</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.319857</td>\n",
       "      <td>-2508.290283</td>\n",
       "      <td>-2387.523193</td>\n",
       "      <td>-4.650035</td>\n",
       "      <td>-4.648216</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.147400</td>\n",
       "      <td>-5.592238</td>\n",
       "      <td>-7.692761</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.100524</td>\n",
       "      <td>-2668.988770</td>\n",
       "      <td>-2509.349365</td>\n",
       "      <td>-4.461812</td>\n",
       "      <td>-4.574172</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.219600</td>\n",
       "      <td>0.287206</td>\n",
       "      <td>-7.624055</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>7.911261</td>\n",
       "      <td>-2660.171387</td>\n",
       "      <td>-2558.669678</td>\n",
       "      <td>-4.217445</td>\n",
       "      <td>-4.168513</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.261500</td>\n",
       "      <td>-4.089805</td>\n",
       "      <td>-11.590117</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>7.500314</td>\n",
       "      <td>-2561.205566</td>\n",
       "      <td>-2509.316895</td>\n",
       "      <td>-4.044028</td>\n",
       "      <td>-4.072822</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.753800</td>\n",
       "      <td>-11.867795</td>\n",
       "      <td>-20.503782</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>8.635985</td>\n",
       "      <td>-2714.646484</td>\n",
       "      <td>-2605.021973</td>\n",
       "      <td>-4.631408</td>\n",
       "      <td>-4.674015</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.916300</td>\n",
       "      <td>-9.156513</td>\n",
       "      <td>-22.363285</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>13.206772</td>\n",
       "      <td>-2681.920898</td>\n",
       "      <td>-2538.767578</td>\n",
       "      <td>-4.751256</td>\n",
       "      <td>-4.794691</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>-2.798518</td>\n",
       "      <td>-19.194332</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>16.395815</td>\n",
       "      <td>-2564.045654</td>\n",
       "      <td>-2537.972412</td>\n",
       "      <td>-4.697979</td>\n",
       "      <td>-4.782566</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>6.759625</td>\n",
       "      <td>-21.737743</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.497370</td>\n",
       "      <td>-2529.057373</td>\n",
       "      <td>-2598.914551</td>\n",
       "      <td>-4.560782</td>\n",
       "      <td>-4.692301</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.246200</td>\n",
       "      <td>6.345956</td>\n",
       "      <td>-22.088821</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>28.434778</td>\n",
       "      <td>-2464.271973</td>\n",
       "      <td>-2587.531006</td>\n",
       "      <td>-4.474179</td>\n",
       "      <td>-4.543868</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.610100</td>\n",
       "      <td>1.915642</td>\n",
       "      <td>-25.032997</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>26.948639</td>\n",
       "      <td>-2554.816406</td>\n",
       "      <td>-2669.541016</td>\n",
       "      <td>-4.531075</td>\n",
       "      <td>-4.612531</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.698300</td>\n",
       "      <td>-1.543622</td>\n",
       "      <td>-33.989979</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>32.446358</td>\n",
       "      <td>-2591.865234</td>\n",
       "      <td>-2766.924805</td>\n",
       "      <td>-4.595987</td>\n",
       "      <td>-4.617951</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.403723</td>\n",
       "      <td>-41.265053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.861334</td>\n",
       "      <td>-2647.361816</td>\n",
       "      <td>-2825.328613</td>\n",
       "      <td>-4.655684</td>\n",
       "      <td>-4.679357</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-10.113770</td>\n",
       "      <td>-44.753723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>34.639957</td>\n",
       "      <td>-2629.455811</td>\n",
       "      <td>-2861.990723</td>\n",
       "      <td>-4.653944</td>\n",
       "      <td>-4.765634</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>-7.571596</td>\n",
       "      <td>-40.558449</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>32.986855</td>\n",
       "      <td>-2698.541504</td>\n",
       "      <td>-2808.578857</td>\n",
       "      <td>-4.639125</td>\n",
       "      <td>-4.734274</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>-7.412705</td>\n",
       "      <td>-46.829048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.416344</td>\n",
       "      <td>-2664.270752</td>\n",
       "      <td>-2837.152832</td>\n",
       "      <td>-4.615863</td>\n",
       "      <td>-4.693789</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.595275</td>\n",
       "      <td>-40.417179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.821903</td>\n",
       "      <td>-2679.108887</td>\n",
       "      <td>-2855.848145</td>\n",
       "      <td>-4.565374</td>\n",
       "      <td>-4.679554</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.861574</td>\n",
       "      <td>-39.501129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.639557</td>\n",
       "      <td>-2631.751953</td>\n",
       "      <td>-2819.520996</td>\n",
       "      <td>-4.541556</td>\n",
       "      <td>-4.588270</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.761018</td>\n",
       "      <td>-41.239208</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.478191</td>\n",
       "      <td>-2626.458252</td>\n",
       "      <td>-2784.822266</td>\n",
       "      <td>-4.483828</td>\n",
       "      <td>-4.628295</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-5.102819</td>\n",
       "      <td>-38.416763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.313942</td>\n",
       "      <td>-2584.507324</td>\n",
       "      <td>-2763.276123</td>\n",
       "      <td>-4.572602</td>\n",
       "      <td>-4.590929</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.958149</td>\n",
       "      <td>-38.401169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.443016</td>\n",
       "      <td>-2572.853760</td>\n",
       "      <td>-2797.786133</td>\n",
       "      <td>-4.562186</td>\n",
       "      <td>-4.641107</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.712317</td>\n",
       "      <td>-39.291218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.578899</td>\n",
       "      <td>-2561.792969</td>\n",
       "      <td>-2792.681641</td>\n",
       "      <td>-4.516206</td>\n",
       "      <td>-4.620252</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.448956</td>\n",
       "      <td>-39.319256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.870296</td>\n",
       "      <td>-2623.233643</td>\n",
       "      <td>-2771.557373</td>\n",
       "      <td>-4.458030</td>\n",
       "      <td>-4.512883</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179730</td>\n",
       "      <td>-37.866119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.045853</td>\n",
       "      <td>-2512.296875</td>\n",
       "      <td>-2752.431396</td>\n",
       "      <td>-4.511570</td>\n",
       "      <td>-4.634753</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO training finished.\n",
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.553 GB.\n",
      "20.564 GB of memory reserved after training.\n",
      "1447.59 seconds used for training.\n",
      "24.13 minutes used for training.\n",
      "Peak reserved memory = 20.564 GB.\n",
      "Peak reserved memory % of max memory = 87.309 %.\n",
      "\n",
      "--- Generating DPO model response (streaming) ---\n",
      "Okay, let's dive into the highly debated topic of pineapple on pizza! Hereâ€™s a breakdown of the pros and cons, considering both the objective and subjective aspects:\n",
      "\n",
      "**Pros (Arguments in favor):**\n",
      "\n",
      "* **Flavor Contrast:** This is the biggest argument. The sweetness and acidity of pineapple provide a fantastic contrast to the savory, salty, and often fatty elements of cheese, ham, and tomato sauce. It's a complex, multi-layered taste experience.\n",
      "* **Acidity Cuts Through Fat:** Pineapple's acidity helps to break down the richness of the cheese and meat, preventing the pizza from feeling too\n",
      "\n",
      "--- End of DPO model response ---\n",
      "DPO LoRA adapters and tokenizer saved to ./gemma-3-4b-dpo-lora\n",
      "\n",
      "Script finished. Remember to replace placeholder DPO dataset with your actual data.\n"
     ]
    }
   ],
   "source": [
    "# Adapted from unsloths's notebooks\n",
    "\n",
    "# import os\n",
    "# if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "#     print(\"Not in Colab. Assuming Unsloth is already installed.\")\n",
    "#     print(\"If not, please install with: pip install unsloth[colab-new]\")\n",
    "# else:\n",
    "#     # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "#     print(\"Installing Unsloth and dependencies for Colab...\")\n",
    "#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "#     !pip install --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "#     print(\"Installation complete.\")\n",
    "\n",
    "from unsloth import FastLanguageModel # Changed from FastModel for DPO consistency\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "max_seq_length = 2048 # Choose any for long context!\n",
    "# Use \"unsloth\" for LoRA optimization to fit 2x larger batch sizes!\n",
    "use_gradient_checkpointing = \"unsloth\" # True or \"unsloth\" for very long context\n",
    "\n",
    "model_id = \"unsloth/gemma-3-4b-it\"\n",
    "\n",
    "# Load Gemma-3 model\n",
    "model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id, # Using the instruction-tuned base\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    "    # You can add dtype=torch.bfloat16 if your GPU supports it for potentially faster training\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # LoRA rank. 8-64 is common. DPO template used 64. Let's try 16.\n",
    "    lora_alpha = 32, # Recommended lora_alpha = 2 * r\n",
    "    lora_dropout = 0.05, # Supports any, but = 0 is optimized for some Unsloth features\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # Target modules for Gemma-3. These are typical for Llama-like architectures.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    use_gradient_checkpointing = use_gradient_checkpointing,\n",
    "    random_state = 3407,\n",
    "    max_seq_length = max_seq_length,\n",
    ")\n",
    "\n",
    "# Set up chat template for Gemma-3\n",
    "# DPOTrainer will use this to format prompts and responses\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    "    # map_eos_token = True, # Gemma-3 tokenizer has an EOS token, so this might not be needed\n",
    ")\n",
    "# It's good practice to set pad_token if it's not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# --- Data Prep for DPO -------------------------------------------------------\n",
    "# set to the dpo pairs dataset that was generated by auto_unslop\n",
    "dpo_file = \"./dpo_pairs_dataset.jsonl\"\n",
    "print(f\"Loading DPO dataset from {dpo_file}\")\n",
    "dpo_dataset = load_dataset(\"json\", data_files=str(dpo_file), split=\"train\")\n",
    "\n",
    "# quick sanity-check â€“ drop any rows that somehow lack the three fields\n",
    "req_cols = {\"prompt\", \"chosen\", \"rejected\"}\n",
    "before   = len(dpo_dataset)\n",
    "dpo_dataset = dpo_dataset.filter(lambda x: all(col in x and x[col] for col in req_cols))\n",
    "after    = len(dpo_dataset)\n",
    "if after == 0:\n",
    "    raise ValueError(\"All rows were filtered out â€“ check dataset contents.\")\n",
    "if after < before:\n",
    "    print(f\"Filtered out {before - after} malformed rows; {after} remain.\")\n",
    "\n",
    "print(f\"DPO dataset ready with {after} samples.\")\n",
    "\n",
    "\n",
    "# --- Train the model with DPO ---\n",
    "# Note: For DPO, you typically don't need a separate eval_dataset during training,\n",
    "# but it can be useful for monitoring.\n",
    "# The `ref_model` is set to None for LoRA, DPOTrainer will handle creating a reference.\n",
    "\n",
    "# Ensure the dataset is not empty\n",
    "if len(dpo_dataset) == 0:\n",
    "    raise ValueError(\"DPO dataset is empty. Please provide a valid dataset.\")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None, # Handled automatically by DPOTrainer for LoRA\n",
    "    train_dataset = dpo_dataset,\n",
    "    # eval_dataset = YOUR_EVAL_DPO_DATASET_HERE, # Optional\n",
    "    tokenizer = tokenizer,\n",
    "    args = DPOConfig(\n",
    "        per_device_train_batch_size = 1, # Adjust based on your GPU memory\n",
    "        gradient_accumulation_steps = 4, # Effective batch size = 1 * 4 = 4\n",
    "        warmup_ratio = 0.1, # Or warmup_steps\n",
    "        num_train_epochs = 3, # For a quick demo. Set to 1-3 for a real run.\n",
    "        # max_steps = 60, # Alternatively, use max_steps for a fixed number of steps\n",
    "        learning_rate = 5e-5, # Common DPO learning rate\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\", # Unsloth optimizes this\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs_gemma3_4b_dpo\",\n",
    "        max_length = max_seq_length,         # Max length of combined prompt + response\n",
    "        max_prompt_length = max_seq_length // 2, # Max length of prompt\n",
    "        beta = 0.1, # DPO beta parameter\n",
    "        report_to = \"none\", # \"wandb\" or \"tensorboard\"\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        # bf16 = True, # Set to True if your GPU supports bfloat16 and you loaded model with bfloat16\n",
    "        # fp16 = False, # Set to True for mixed precision if bf16 is not available (and not using 4bit)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Starting DPO training...\")\n",
    "trainer_stats = dpo_trainer.train()\n",
    "print(\"DPO training finished.\")\n",
    "\n",
    "\n",
    "# --- Show current memory stats (optional) ---\n",
    "if torch.cuda.is_available():\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.memory_reserved() / 1024 / 1024 / 1024, 3) # Corrected: use memory_reserved\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved after training.\") # This will be peak if called after train\n",
    "\n",
    "    # Show final memory and time stats\n",
    "    if hasattr(trainer_stats, 'metrics') and 'train_runtime' in trainer_stats.metrics:\n",
    "        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        # Note: start_gpu_memory was captured *after* model loading.\n",
    "        # A more accurate \"used_memory_for_lora\" would require capturing memory before model load and after.\n",
    "        # For simplicity, we'll just show peak reserved.\n",
    "        used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "        print(f\"{trainer_stats.metrics['train_runtime']:.2f} seconds used for training.\")\n",
    "        print(f\"{trainer_stats.metrics['train_runtime']/60:.2f} minutes used for training.\")\n",
    "        print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "        print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "else:\n",
    "    print(\"CUDA not available. Memory stats not shown.\")\n",
    "\n",
    "\n",
    "# --- Inference after DPO ---\n",
    "# For inference, make sure the tokenizer has the chat template for Gemma-3\n",
    "# The model is already LoRA-adapted.\n",
    "\n",
    "# Reload tokenizer with chat template if needed (should be already set)\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template = \"gemma-3\",\n",
    "# )\n",
    "\n",
    "# Example prompt\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What are the pros and cons of pineapple on pizza?\"\n",
    "}]\n",
    "\n",
    "# Apply chat template for generation\n",
    "# For DPO, the model has learned from preferences, so it should generate better responses.\n",
    "# The prompt format should match what it saw during training (user turn).\n",
    "# `add_generation_prompt=True` adds the necessary tokens to signal the model to start generating.\n",
    "# For Gemma-3, this means it will end with `<start_of_turn>model\\n`\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Crucial for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "\n",
    "print(\"\\n--- Generating DPO model response (streaming) ---\")\n",
    "_ = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens = 128, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 0.7, # Slightly lower temperature for more focused output after DPO\n",
    "    top_p = 0.95,\n",
    "    top_k = 64,\n",
    "    streamer = text_streamer,\n",
    "    pad_token_id = tokenizer.eos_token_id # Important for generation\n",
    ")\n",
    "print(\"\\n--- End of DPO model response ---\")\n",
    "\n",
    "print(\"\\nScript finished. Remember to replace placeholder DPO dataset with your actual data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa7c3a1-f8f5-40d1-94df-4b9445adb3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating DPO model response (streaming) ---\n",
      "The air in the Sterling Federal Building was a specific kind of heavy, a blend of legal documents, nervous energy, and the low, constant hum of fluorescent lights. It pressed down on me, a physical weight, and it was nothing compared to the weight on Leoâ€™s shoulders. He was a study in controlled anxiety, a fact Iâ€™d become intimately familiar with over the last decade. Leo could read minds. Not with a dramatic, telepathic projection, but a quiet, internal awareness, a constant, low-level reception of thoughts and feelings. It was a gift, a curse, and, as heâ€™d always dreamed, a potential weapon against the bad guys. \n",
      "\n",
      "Heâ€™d applied to the FBI, specifically the Behavioral Analysis Unit, a path heâ€™d mapped out with the single-minded determination of a man whoâ€™d spent years processing the internal monologues of criminals. I, on the other hand, was a freelance writer, content with observing the world from a comfortable distance. Iâ€™d agreed to go with him to this interview, a silent, supportive presence, a buffer against the potential overload of a dozen different minds in one room. \n",
      "\n",
      "The interviewers, a woman named Agent Davies and a man who looked like heâ€™d been carved from granite, Mr. Henderson, were efficient and professional. Theyâ€™d started with the standard questions: background, motivation, experience. Leo answered each one with a practiced calm, his responses measured and considered. Heâ€™d been preparing for this, of course. Heâ€™d run simulations, practiced scenarios, even had a friend, a retired police officer, walk him through a mock interview. \n",
      "\n",
      "But as the questions became more specific, delving into psychological profiles and investigative techniques, Leo started to change. His usual, steady hands began to tremble slightly. The controlled expression on his face started to crack, a minute bead of sweat forming on his forehead. I could feel it, a low, rising tide of panic. He was being bombarded. Not with a single, clear thought, but a cacophony of opinions, judgments, and half-formed assessments from the two interviewers.\n",
      "\n",
      "Davies was a classic, a woman whoâ€™d clearly seen a lot and wasnâ€™t impressed. She was assessing him, measuring his potential, and, I realized with a jolt, judging him. Henderson, the granite man, was assessing the *threat* â€“ not a threat to the agency, but a threat to the established order, to the way things were. They were both, in their own ways, trying to figure out if he was a useful tool or a liability.\n",
      "\n",
      "The pressure was building. Leoâ€™s breathing grew shallow, and his eyes, a normally warm, hazel green, were now wide and focused, a dark pool of concentration. He was fighting, trying to block out the incoming thoughts, a skill heâ€™d developed over years of practice, but today,\n",
      "\n",
      "--- End of DPO model response ---\n"
     ]
    }
   ],
   "source": [
    "p = \"In this world, salaries are determined by the desirability of the work: if everybody wants to do the job and it's fun, it pays minimum wage. But if it's hard or awful work that nobody wants to do, the pay is high. You decide to apply for the highest-paying job in the world.\"\n",
    "p = \"Write 1000 word short story to this prompt: A close friend of yours can read minds. It was their dream to work for the FBI or CIA to catch bad guys. You accompanied them to their first interview, but instead they walk straight back out. They whisper to you to walk calmly out to the car and not to say a word or make eye contact, act calm.\"\n",
    "\n",
    "# Example prompt\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": p\n",
    "}]\n",
    "\n",
    "# Apply chat template for generation\n",
    "# For DPO, the model has learned from preferences, so it should generate better responses.\n",
    "# The prompt format should match what it saw during training (user turn).\n",
    "# `add_generation_prompt=True` adds the necessary tokens to signal the model to start generating.\n",
    "# For Gemma-3, this means it will end with `<start_of_turn>model\\n`\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Crucial for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "\n",
    "print(\"\\n--- Generating DPO model response (streaming) ---\")\n",
    "_ = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens = 600, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 0.7, # Slightly lower temperature for more focused output after DPO\n",
    "    top_p = 0.95,\n",
    "    top_k = 64,\n",
    "    streamer = text_streamer,\n",
    "    pad_token_id = tokenizer.eos_token_id # Important for generation\n",
    ")\n",
    "print(\"\\n--- End of DPO model response ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc07d814-0692-46a5-9e90-11f1ec4faf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO LoRA adapters and tokenizer saved to ./gemma-3-4b-dpo-lora\n",
      "Downloading safetensors index for unsloth/gemma-3-4b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873cbe0a40be43198a9ce6e570ceba7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e543783fc5734ca5ac7f9b3f3805cc9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:51<00:51, 51.90s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218b12645246451aa592063eb420e112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:34<00:00, 47.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 16-bit DPO model saved to ./gemma-3-4b-dpo-unslopped\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Saving the DPO-finetuned model (LoRA adapters) ---\n",
    "dpo_model_save_path = \"gemma-3-4b-dpo-lora\"\n",
    "dpo_trainer.save_model(dpo_model_save_path) # Saves LoRA adapters\n",
    "tokenizer.save_pretrained(dpo_model_save_path)\n",
    "print(f\"DPO LoRA adapters and tokenizer saved to ./{dpo_model_save_path}\")\n",
    "\n",
    "# To load the LoRA adapters later for inference:\n",
    "if False: # Set to True to test loading\n",
    "    from unsloth import FastLanguageModel\n",
    "    loaded_model, loaded_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = dpo_model_save_path, # Path to your saved LoRA adapters\n",
    "        max_seq_length = max_seq_length,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "    # Now `loaded_model` is ready for inference.\n",
    "    # Ensure tokenizer has chat template\n",
    "    loaded_tokenizer = get_chat_template(\n",
    "        loaded_tokenizer,\n",
    "        chat_template = \"gemma-3\",\n",
    "    )\n",
    "    if loaded_tokenizer.pad_token is None:\n",
    "        loaded_tokenizer.pad_token = loaded_tokenizer.eos_token\n",
    "\n",
    "    print(\"\\n--- Generating response from loaded DPO LoRA model (streaming) ---\")\n",
    "    _ = loaded_model.generate(\n",
    "        inputs, # Using the same inputs as before\n",
    "        max_new_tokens = 128,\n",
    "        temperature = 0.7, top_p = 0.95, top_k = 64,\n",
    "        streamer = TextStreamer(loaded_tokenizer, skip_prompt = True),\n",
    "        pad_token_id = loaded_tokenizer.eos_token_id\n",
    "    )\n",
    "    print(\"\\n--- End of loaded DPO model response ---\")\n",
    "\n",
    "\n",
    "# --- Saving to float16 for VLLM or other deployments (merged model) ---\n",
    "if True: # Change to True to save merged finetune!\n",
    "    # Merges LoRA adapters into the base model and saves\n",
    "    # This creates a standalone model, not just adapters\n",
    "    merged_model_path = \"gemma-3-4b-dpo-unslopped\"\n",
    "    model.save_pretrained_merged(merged_model_path, tokenizer, save_method = \"merged_16bit\")\n",
    "    print(f\"Merged 16-bit DPO model saved to ./{merged_model_path}\")\n",
    "    # For GGUF, you can then convert this merged model or use Unsloth's direct GGUF saving\n",
    "    # model.push_to_hub_merged(\"YOUR_HF_USERNAME/gemma-3-4b-dpo-merged\", tokenizer, save_method = \"merged_16bit\", token = \"YOUR_HF_TOKEN\")\n",
    "\n",
    "# --- GGUF / llama.cpp Conversion ---\n",
    "if False: # Change to True to save to GGUF\n",
    "    # Saves the LoRA model directly to GGUF by first merging.\n",
    "    # Quantization types: \"Q8_0\", \"F16\", \"BF16\", \"Q4_K_M\", \"Q5_K_M\" etc.\n",
    "    gguf_model_path = \"gemma-3-4b-dpo-gguf\"\n",
    "    model.save_pretrained_gguf(gguf_model_path, tokenizer, quantization_method = \"q8_0\")\n",
    "    print(f\"GGUF (Q8_0) DPO model saved to ./{gguf_model_path}.gguf\") # Unsloth adds .gguf\n",
    "    # model.push_to_hub_gguf(\"YOUR_HF_USERNAME/gemma-3-4b-dpo-gguf\", tokenizer, quantization_method = \"q8_0\", token = \"YOUR_HF_TOKEN\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
